{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b32ec4e1",
   "metadata": {},
   "source": [
    "# Leveraging PINNs For Multi-Dimensional Pricing Problems \n",
    "#### Author: JP Melo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408abab0",
   "metadata": {},
   "source": [
    "This thesis is focused on the application of Physics-Informed Neural Networks (PINNs) to solve multi-dimensional pricing problems in finance. The equation we attempt to solve is described below.\n",
    "\n",
    "### **Equation**\n",
    "First, we define the scaled variables \n",
    "$$\n",
    "  x_i \\;=\\; \\ln\\!\\Bigl(\\frac{S_i}{K}\\Bigr),\n",
    "  \\quad \n",
    "  u(\\tau, x_1, \\dots, x_d) \\;=\\; \\frac{V\\bigl(t,S_1,\\dots,S_d\\bigr)}{K}\n",
    "  \\quad\\text{with}\\quad \n",
    "  \\tau \\;=\\; T - t.\n",
    "$$\n",
    "\n",
    "Under risk-neutral pricing in backward time $\\tau$, the function \n",
    "$u(\\tau, x_1, \\dots, x_d)$ satisfies the PDE\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial \\tau} \\;=\\;\n",
    "\\frac{1}{2}\\sum_{i=1}^d \n",
    "\\sigma_i^{2}\\!\\left(\n",
    "\\frac{\\partial^{2}u}{\\partial x_i^{2}}\n",
    "-\\frac{\\partial u}{\\partial x_i}\\right)\n",
    "\\;+\\;\n",
    "\\frac{1}{2}\\sum_{i=1}^d\\sum_{j=1}^d\n",
    "\\sigma_i\\sigma_j\\rho_{ij}\\,\n",
    "\\frac{\\partial^{2}u}{\\partial x_i\\,\\partial x_j}\n",
    "\\;+\\;\n",
    "r\\sum_{i=1}^d\\frac{\\partial u}{\\partial x_i}\n",
    "\\;-\\;\n",
    "r\\,u .\n",
    "$$\n",
    "\n",
    "### **Boundary Conditions**\n",
    "\n",
    "**Bottom boundary**  \n",
    "For very small $S_i$ i.e. $x_i \\to -\\infty$, one commonly imposes (1 asset)\n",
    "$$\n",
    "    -\\frac{\\partial u}{\\partial \\tau} - ru \\;=\\; 0.\n",
    "$$\n",
    "\n",
    "For the multi-asset case, the k'th lower boundary condition is given by\n",
    "\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial \\tau} \\;=\\;\n",
    "\\frac{1}{2}\\sum_{\\substack{i=1 \\\\ i\\neq k}}^d \n",
    "\\sigma_i^{2}\\!\\left(\n",
    "\\frac{\\partial^{2}u}{\\partial x_i^{2}}\n",
    "-\\frac{\\partial u}{\\partial x_i}\\right)\n",
    "\\;+\\;\n",
    "\\frac{1}{2}\\sum_{\\substack{i=1 \\\\ i\\neq k}}^d\n",
    "      \\sum_{\\substack{j=1 \\\\ j\\neq k}}^d\n",
    "\\sigma_i\\sigma_j\\rho_{ij}\\,\n",
    "\\frac{\\partial^{2}u}{\\partial x_i\\,\\partial x_j}\n",
    "\\;+\\;\n",
    "r\\sum_{\\substack{i=1 \\\\ i\\neq k}}^d\n",
    "\\frac{\\partial u}{\\partial x_i}\n",
    "\\;-\\;\n",
    "r\\,u .\n",
    "$$\n",
    "\n",
    "**Top boundary**  \n",
    "For very large $S_i$ i.e. $x_i \\to +\\infty$, assume asymptotically linear behavior in $S_i$,\n",
    "which translates to the following expression in the dimensionless case (1 asset):\n",
    "$$\n",
    "   \\frac{\\partial^2 u}{\\partial x_i^2}-\\frac{\\partial u}{\\partial x_i} = 0\n",
    "$$\n",
    "\n",
    "The generalization for the multi-asset case is straightforward is presented below:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial \\tau} \\;=\\;\n",
    "\\frac{1}{2}\\sum_{\\substack{i=1 \\\\ i\\neq k}}^d \n",
    "\\sigma_i^{2}\\!\\left(\n",
    "\\frac{\\partial^{2}u}{\\partial x_i^{2}}\n",
    "-\\frac{\\partial u}{\\partial x_i}\\right)\n",
    "\\;+\\;\n",
    "\\frac{1}{2}\\sum_{\\substack{i=1 \\\\ i\\neq k}}^d\n",
    "      \\sum_{\\substack{j=1 \\\\ j\\neq k}}^d\n",
    "\\sigma_i\\sigma_j\\rho_{ij}\\,\n",
    "\\frac{\\partial^{2}u}{\\partial x_i\\,\\partial x_j}\n",
    "\\;+\\;\n",
    "r\\sum_{i=1}^d\\frac{\\partial u}{\\partial x_i}\n",
    "\\;-\\;\n",
    "r\\,u .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50527de",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d74c022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josemelo/Desktop/master/tesis/codes/.conda/lib/python3.11/site-packages/kfac/base_preconditioner.py:15: UserWarning: NVIDIA Apex is not installed or was not installed with --cpp_ext. Falling back to PyTorch flatten and unflatten.\n",
      "  from kfac.distributed import get_rank\n"
     ]
    }
   ],
   "source": [
    "from derpinns.nn import *\n",
    "from derpinns.utils import *\n",
    "from derpinns.trainer import *\n",
    "import torch\n",
    "import kfac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc70ad0c",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1176864d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NN(\n",
       "  (hidden_layers): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (5): Tanh()\n",
       "  )\n",
       "  (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fix seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Global parameters\n",
    "assets = 3\n",
    "sampler = \"pseudo\"               # [\"pseudo\", \"LHS\", \"Halton\", \"Hammersley\", \"Sobol\"]:\n",
    "nn_shape = \"64x3\"               # n_assets input layer + 64 neurons, 3 hidden layers + 1 output layer\n",
    "device = torch.device(\"cpu\")    # cpu, cuda or mps\n",
    "dtype = torch.float32\n",
    "\n",
    "# Define option valuation params\n",
    "params = OptionParameters(\n",
    "    n_assets=assets,\n",
    "    tau=1.0,\n",
    "    sigma=np.array([0.2] * assets),\n",
    "    rho=np.eye(assets) + 0.25 * (np.ones((assets, assets)) - np.eye(assets)),\n",
    "    r=0.05,\n",
    "    strike=100,\n",
    "    payoff=payoff\n",
    ")\n",
    "\n",
    "# Build the net to be used\n",
    "model = build_nn(\n",
    "    nn_shape=nn_shape,\n",
    "    input_dim=assets,\n",
    "    dtype=torch.float32\n",
    ").apply(weights_init).to(device)\n",
    "\n",
    "### Other possible net models\n",
    "# model = NNAnzats(n_layers=3, input_dim=assets+1,hidden_dim=64, output_dim=1).apply(weights_init).to(device)\n",
    "# model = SPINN(n_layers=3, input_dim=assets+1, hidden_dim=32, output_dim=1).apply(weights_init).to(device)\n",
    "# model = NNWithFourier(n_layers=3, input_dim=assets+1, hidden_dim=64, output_dim=1).apply(weights_init).to(device)\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdffad9",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "For training, different optimizers are used in order to get better accuracy as stated in [this article](https://arxiv.org/pdf/2402.01868)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c5452c",
   "metadata": {},
   "source": [
    "### Adam Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a7c1354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the training parameters\n",
    "batch_size = 1_000\n",
    "total_iter = 500\n",
    "boundary_samples = 20_000\n",
    "interior_samples = boundary_samples*assets*2\n",
    "initial_samples = boundary_samples*assets*2\n",
    "\n",
    "# Create dataset to traing over\n",
    "dataset = SampledDataset(\n",
    "    params, interior_samples, initial_samples, boundary_samples, sampler, dtype, device, seed=0)\n",
    "\n",
    "# Set optimizer and training function\n",
    "# 1e-2 is big enought to reach a reasonable min in few steps\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, amsgrad=True)\n",
    "preconditioner = kfac.preconditioner.KFACPreconditioner(model)\n",
    "\n",
    "# # Set the training function\n",
    "closure = DimlessBS()\\\n",
    "    .with_dataset(dataset, loader_opts={'batch_size': batch_size, \"shuffle\": True, \"pin_memory\": True})\\\n",
    "    .with_model(model)\\\n",
    "    .with_device(device)\\\n",
    "    .with_dtype(dtype)\n",
    "\n",
    "trainer = PINNTrainer()\\\n",
    "    .with_optimizer(optimizer)\\\n",
    "    .with_device(device)\\\n",
    "    .with_dtype(dtype)\\\n",
    "    .with_training_step(closure)\\\n",
    "    .with_preconditioner(preconditioner)\\\n",
    "    .with_epochs(total_iter)\\\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "524726b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m state \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mclosure\u001b[38;5;241m.\u001b[39mget_state()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplot_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmooth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmooth_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/master/tesis/codes/derivative_pinns/src/derpinns/utils.py:51\u001b[0m, in \u001b[0;36mplot_loss\u001b[0;34m(loss_history, save_path, backend, fig_size, smooth, smooth_window)\u001b[0m\n\u001b[1;32m     48\u001b[0m initial \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(loss_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitial_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m])   \u001b[38;5;66;03m# fixed typo\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m smooth:\n\u001b[0;32m---> 51\u001b[0m     interior \u001b[38;5;241m=\u001b[39m \u001b[43m_moving_average\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmooth_window\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     boundary \u001b[38;5;241m=\u001b[39m _moving_average(boundary, smooth_window)\n\u001b[1;32m     53\u001b[0m     initial \u001b[38;5;241m=\u001b[39m _moving_average(initial,  smooth_window)\n",
      "File \u001b[0;32m~/Desktop/master/tesis/codes/derivative_pinns/src/derpinns/utils.py:19\u001b[0m, in \u001b[0;36m_moving_average\u001b[0;34m(arr, window)\u001b[0m\n\u001b[1;32m     17\u001b[0m smoothed \u001b[38;5;241m=\u001b[39m (cumsum[window:] \u001b[38;5;241m-\u001b[39m cumsum[:\u001b[38;5;241m-\u001b[39mwindow]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(window)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# pad the left side so lengths match\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m left_pad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull(window \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[43msmoothed\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate([left_pad, smoothed])\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "state = trainer.closure.get_state()\n",
    "plot_loss(state, smooth=True, smooth_window=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaec2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 Error:  5.784354\n"
     ]
    }
   ],
   "source": [
    "results = compare_with_mc(model, params, n_prices=200,\n",
    "                          n_simulations=10_000, dtype=dtype, device=device, seed=42)['l2_rel_error']\n",
    "print(\"L2 Error: \", results*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f1c27f",
   "metadata": {},
   "source": [
    "### LBFGS Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94ae5cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interior Loss: 0.007596821058541536\n",
      "Boundary Loss: 0.03973814845085144\n",
      "Inital Condition Loss: 8.126863479614258\n",
      "Total Loss: 8.17419844912365\n",
      "----------------------------------------\n",
      "Interior Loss: 0.6102046966552734\n",
      "Boundary Loss: 3.825228691101074\n",
      "Inital Condition Loss: 74.61697387695312\n",
      "Total Loss: 79.05240726470947\n",
      "----------------------------------------\n",
      "Interior Loss: 0.4467941224575043\n",
      "Boundary Loss: 2.787994384765625\n",
      "Inital Condition Loss: 60.9835090637207\n",
      "Total Loss: 64.21829757094383\n",
      "----------------------------------------\n",
      "Interior Loss: 0.3684530556201935\n",
      "Boundary Loss: 2.286050319671631\n",
      "Inital Condition Loss: 53.7800407409668\n",
      "Total Loss: 56.43454411625862\n",
      "----------------------------------------\n",
      "Interior Loss: 0.33100342750549316\n",
      "Boundary Loss: 2.0448763370513916\n",
      "Inital Condition Loss: 49.951026916503906\n",
      "Total Loss: 52.32690668106079\n",
      "----------------------------------------\n",
      "Interior Loss: 0.3128088414669037\n",
      "Boundary Loss: 1.9271972179412842\n",
      "Inital Condition Loss: 47.95338439941406\n",
      "Total Loss: 50.19339045882225\n",
      "----------------------------------------\n",
      "Interior Loss: 0.3038513958454132\n",
      "Boundary Loss: 1.86909019947052\n",
      "Inital Condition Loss: 46.92961502075195\n",
      "Total Loss: 49.102556616067886\n",
      "----------------------------------------\n",
      "Interior Loss: 0.29940786957740784\n",
      "Boundary Loss: 1.8402149677276611\n",
      "Inital Condition Loss: 46.4109001159668\n",
      "Total Loss: 48.550522953271866\n",
      "----------------------------------------\n",
      "Interior Loss: 0.2971949279308319\n",
      "Boundary Loss: 1.8258211612701416\n",
      "Inital Condition Loss: 46.14976119995117\n",
      "Total Loss: 48.272777289152145\n",
      "----------------------------------------\n",
      "Interior Loss: 0.29609066247940063\n",
      "Boundary Loss: 1.8186352252960205\n",
      "Inital Condition Loss: 46.01873779296875\n",
      "Total Loss: 48.13346368074417\n",
      "----------------------------------------\n",
      "Interior Loss: 0.2955390512943268\n",
      "Boundary Loss: 1.8150447607040405\n",
      "Inital Condition Loss: 45.95310974121094\n",
      "Total Loss: 48.063693553209305\n",
      "----------------------------------------\n",
      "Interior Loss: 0.2952633798122406\n",
      "Boundary Loss: 1.8132500648498535\n",
      "Inital Condition Loss: 45.920265197753906\n",
      "Total Loss: 48.028778642416\n",
      "----------------------------------------\n",
      "Interior Loss: 0.2951256036758423\n",
      "Boundary Loss: 1.8123531341552734\n",
      "Inital Condition Loss: 45.903839111328125\n",
      "Total Loss: 48.01131784915924\n",
      "----------------------------------------\n",
      "Interior Loss: 0.2950567305088043\n",
      "Boundary Loss: 1.8119046688079834\n",
      "Inital Condition Loss: 45.895626068115234\n",
      "Total Loss: 48.00258746743202\n",
      "----------------------------------------\n",
      "Interior Loss: 0.29502230882644653\n",
      "Boundary Loss: 1.8116803169250488\n",
      "Inital Condition Loss: 45.89151382446289\n",
      "Total Loss: 47.998216450214386\n",
      "----------------------------------------\n",
      "Interior Loss: 0.29500505328178406\n",
      "Boundary Loss: 1.811568021774292\n",
      "Inital Condition Loss: 45.88945770263672\n",
      "Total Loss: 47.996030777692795\n",
      "----------------------------------------\n",
      "Interior Loss: 0.2949964702129364\n",
      "Boundary Loss: 1.8115122318267822\n",
      "Inital Condition Loss: 45.888427734375\n",
      "Total Loss: 47.99493643641472\n",
      "----------------------------------------\n",
      "Interior Loss: 0.2949921190738678\n",
      "Boundary Loss: 1.8114840984344482\n",
      "Inital Condition Loss: 45.887916564941406\n",
      "Total Loss: 47.99439278244972\n",
      "----------------------------------------\n",
      "Interior Loss: 0.29499000310897827\n",
      "Boundary Loss: 1.8114702701568604\n",
      "Inital Condition Loss: 45.887664794921875\n",
      "Total Loss: 47.994125068187714\n",
      "----------------------------------------\n",
      "Interior Loss: 0.29498887062072754\n",
      "Boundary Loss: 1.8114631175994873\n",
      "Inital Condition Loss: 45.88753128051758\n",
      "Total Loss: 47.99398326873779\n",
      "----------------------------------------\n",
      "Interior Loss: 0.29498836398124695\n",
      "Boundary Loss: 1.8114595413208008\n",
      "Inital Condition Loss: 45.88747024536133\n",
      "Total Loss: 47.993918150663376\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'pbar' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m closure \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mwith_dataset(\n\u001b[1;32m     17\u001b[0m     dataset, loader_opts\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: batch_size, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshuffle\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpin_memory\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m})\n\u001b[1;32m     19\u001b[0m trainer \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mwith_optimizer(optimizer)\u001b[38;5;241m.\u001b[39mwith_training_step(closure)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m state \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mget_state()\n\u001b[1;32m     23\u001b[0m plot_loss(state, smooth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/master/tesis/codes/derivative_pinns/src/derpinns/trainer.py:103\u001b[0m, in \u001b[0;36mPINNTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclosure\u001b[38;5;241m.\u001b[39mnext_batch()\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep(closure)\n\u001b[0;32m--> 103\u001b[0m     \u001b[43mpbar\u001b[49m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, Adahessian):\n\u001b[1;32m    106\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdahessian training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'pbar' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "boundary_samples = 1_000\n",
    "interior_samples = boundary_samples*assets*2\n",
    "initial_samples = boundary_samples*assets*2\n",
    "\n",
    "# We create new samples\n",
    "dataset = SampledDataset(\n",
    "    params, interior_samples, initial_samples, boundary_samples, sampler, dtype, device, seed=0)\n",
    "\n",
    "optimizer = LBFGS(\n",
    "    model.parameters(),\n",
    "    max_eval=1_000,\n",
    "    max_iter=5_000,\n",
    "    line_search_fn=\"strong_wolfe\",\n",
    ")\n",
    "batch_size = len(dataset) # we use all samples\n",
    "\n",
    "closure = closure.with_dataset(\n",
    "    dataset, loader_opts={'batch_size': batch_size, \"shuffle\": False, \"pin_memory\": True})\n",
    "\n",
    "trainer = trainer.with_optimizer(optimizer).with_training_step(closure)\n",
    "trainer.train()\n",
    "\n",
    "state = closure.get_state()\n",
    "plot_loss(state, smooth=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824c6b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 Error:  0.4932503\n"
     ]
    }
   ],
   "source": [
    "results = compare_with_mc(model, params, n_prices=200,\n",
    "                          n_simulations=10_000, dtype=dtype, device=device, seed=42)['l2_rel_error']\n",
    "print(\"L2 Error (%): \", results*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
