{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b32ec4e1",
   "metadata": {},
   "source": [
    "# Do Nets Matter?\n",
    "#### Author: JP Melo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408abab0",
   "metadata": {},
   "source": [
    "In this file we explore how different shapes of nets affect performance and accuracy of the model. We will use the same dataset as before, but we will create different nets with different shapes and compare their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50527de",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d74c022",
   "metadata": {},
   "outputs": [],
   "source": [
    "from derpinns.nn import *\n",
    "from derpinns.utils import *\n",
    "from derpinns.trainer import *\n",
    "import torch\n",
    "import kfac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc70ad0c",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1176864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Global parameters\n",
    "assets = 2\n",
    "\n",
    "sampler = \"pseudo\"\n",
    "nn_shape = \"64x3\"\n",
    "device = torch.device(\"cpu\")\n",
    "dtype = torch.float32\n",
    "\n",
    "# Define option valuation params\n",
    "params = OptionParameters(\n",
    "    n_assets=assets,\n",
    "    tau=1.0,\n",
    "    sigma=np.array([0.2] * assets),\n",
    "    rho=np.eye(assets) + 0.25 * (np.ones((assets, assets)) - np.eye(assets)),\n",
    "    r=0.05,\n",
    "    strike=100,\n",
    "    payoff=payoff\n",
    ")\n",
    "\n",
    "# Define the number of samples to be used in each training stage\n",
    "\n",
    "adam_batch_size = 500\n",
    "adam_total_iter = 500\n",
    "adam_boundary_samples = 20_000\n",
    "adam_interior_samples = adam_boundary_samples*assets*2\n",
    "adam_initial_samples = adam_boundary_samples*assets*2\n",
    "\n",
    "lbfgs_boundary_samples = 1_000\n",
    "lbfgs_interior_samples = lbfgs_boundary_samples*assets*2\n",
    "lbfgs_initial_samples = lbfgs_boundary_samples*assets*2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdffad9",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "In this case, we use the full training pipeline as the idea is to analize the expressability of the model and not the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c5452c",
   "metadata": {},
   "source": [
    "### Vanilla NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7c1354",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_nn(\n",
    "    nn_shape=nn_shape,\n",
    "    input_dim=assets,\n",
    "    dtype=torch.float32\n",
    ").apply(weights_init).to(device)\n",
    "model.train()\n",
    "\n",
    "dataset = SampledDataset(\n",
    "    params, adam_interior_samples, adam_initial_samples, adam_boundary_samples, sampler, dtype, device, seed=0)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, amsgrad=True)\n",
    "preconditioner = kfac.preconditioner.KFACPreconditioner(model)\n",
    "\n",
    "closure = DimlessBS()\\\n",
    "    .with_dataset(dataset, loader_opts={'batch_size': adam_batch_size, \"shuffle\": True, \"pin_memory\": True})\\\n",
    "    .with_model(model)\\\n",
    "    .with_device(device)\\\n",
    "    .with_dtype(dtype)\n",
    "\n",
    "trainer = PINNTrainer()\\\n",
    "    .with_optimizer(optimizer)\\\n",
    "    .with_device(device)\\\n",
    "    .with_dtype(dtype)\\\n",
    "    .with_training_step(closure)\\\n",
    "    .with_preconditioner(preconditioner)\\\n",
    "    .with_epochs(adam_total_iter)\\\n",
    "\n",
    "# first training stage\n",
    "trainer.train()\n",
    "\n",
    "# we create new samples for the second stage\n",
    "dataset = SampledDataset(\n",
    "    params, lbfgs_interior_samples, lbfgs_initial_samples, lbfgs_boundary_samples, sampler, dtype, device, seed=0)\n",
    "\n",
    "optimizer = LBFGS(\n",
    "    model.parameters(),\n",
    "    max_eval=1_000,\n",
    "    max_iter=5_000,\n",
    "    line_search_fn=\"strong_wolfe\",\n",
    ")\n",
    "batch_size = len(dataset)  # we use all samples\n",
    "\n",
    "closure = closure.with_dataset(\n",
    "    dataset, loader_opts={'batch_size': batch_size, \"shuffle\": False, \"pin_memory\": True})\n",
    "\n",
    "trainer = trainer.with_optimizer(optimizer).with_training_step(closure)\n",
    "\n",
    "# second training stage\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaec2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_vanilla = trainer.closure.get_state()\n",
    "plot_loss(with_vanilla, smooth=True, smooth_window=50)\n",
    "\n",
    "vanilla_l2 = compare_with_mc(model, params, n_prices=200,\n",
    "                             n_simulations=10_000, dtype=dtype, device=device, seed=42)['l2_rel_error']\n",
    "print(\"L2 Error: \", vanilla_l2*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02b9ad8",
   "metadata": {},
   "source": [
    "## With SPINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1698ce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the net to be used\n",
    "model = SPINN(n_layers=3, input_dim=assets+1, hidden_dim=10,\n",
    "output_dim=10, dtype=dtype).apply(weights_init).to(device)\n",
    "\n",
    "dataset = SampledDataset(\n",
    "    params, adam_interior_samples, adam_initial_samples, adam_boundary_samples, sampler, dtype, device, seed=0)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, amsgrad=True)\n",
    "preconditioner = kfac.preconditioner.KFACPreconditioner(model)\n",
    "\n",
    "closure = DimlessBS()\\\n",
    "    .with_dataset(dataset, loader_opts={'batch_size': adam_batch_size, \"shuffle\": True, \"pin_memory\": True})\\\n",
    "    .with_model(model)\\\n",
    "    .with_device(device)\\\n",
    "    .with_dtype(dtype)\n",
    "\n",
    "trainer = PINNTrainer()\\\n",
    "    .with_optimizer(optimizer)\\\n",
    "    .with_device(device)\\\n",
    "    .with_dtype(dtype)\\\n",
    "    .with_training_step(closure)\\\n",
    "    .with_preconditioner(preconditioner)\\\n",
    "    .with_epochs(adam_total_iter)\\\n",
    "\n",
    "# first training stage\n",
    "trainer.train()\n",
    "\n",
    "# we create new samples for the second stage\n",
    "dataset = SampledDataset(\n",
    "    params, lbfgs_interior_samples, lbfgs_initial_samples, lbfgs_boundary_samples, sampler, dtype, device, seed=0)\n",
    "\n",
    "optimizer = LBFGS(\n",
    "    model.parameters(),\n",
    "    max_eval=1_000,\n",
    "    max_iter=5_000,\n",
    "    line_search_fn=\"strong_wolfe\",\n",
    ")\n",
    "batch_size = len(dataset)  # we use all samples\n",
    "\n",
    "closure = closure.with_dataset(\n",
    "    dataset, loader_opts={'batch_size': batch_size, \"shuffle\": False, \"pin_memory\": True})\n",
    "\n",
    "trainer = trainer.with_optimizer(optimizer).with_training_step(closure)\n",
    "\n",
    "# second training stage\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390f1c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_spinn = trainer.closure.get_state()\n",
    "plot_loss(with_spinn, smooth=True, smooth_window=50)\n",
    "\n",
    "spinn_l2 = compare_with_mc(model, params, n_prices=200,\n",
    "                           n_simulations=10_000, dtype=dtype, device=device, seed=42)['l2_rel_error']\n",
    "print(\"L2 Error: \", spinn_l2*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9a0f69",
   "metadata": {},
   "source": [
    "## With NN+Anzats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cea1895",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NNAnzats(n_layers=3, input_dim=assets+1, hidden_dim=64,\n",
    "                 output_dim=1, dtype=dtype).apply(weights_init).to(device)\n",
    "\n",
    "dataset = SampledDataset(\n",
    "    params, adam_interior_samples, adam_initial_samples, adam_boundary_samples, sampler, dtype, device, seed=0)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, amsgrad=True)\n",
    "preconditioner = kfac.preconditioner.KFACPreconditioner(model)\n",
    "\n",
    "closure = DimlessBS()\\\n",
    "    .with_dataset(dataset, loader_opts={'batch_size': adam_batch_size, \"shuffle\": True, \"pin_memory\": True})\\\n",
    "    .with_model(model)\\\n",
    "    .with_device(device)\\\n",
    "    .with_dtype(dtype)\n",
    "\n",
    "trainer = PINNTrainer()\\\n",
    "    .with_optimizer(optimizer)\\\n",
    "    .with_device(device)\\\n",
    "    .with_dtype(dtype)\\\n",
    "    .with_training_step(closure)\\\n",
    "    .with_preconditioner(preconditioner)\\\n",
    "    .with_epochs(adam_total_iter)\\\n",
    "\n",
    "# first training stage\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6491f14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_anzats = trainer.closure.get_state()\n",
    "plot_loss(with_anzats, smooth=True, smooth_window=50)\n",
    "\n",
    "anzats_l2 = compare_with_mc(model, params, n_prices=200,\n",
    "                            n_simulations=10_000, dtype=dtype, device=device, seed=42)['l2_rel_error']\n",
    "print(\"L2 Error: \", anzats_l2*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24c7aac",
   "metadata": {},
   "source": [
    "6.2328877/3.9504097"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde0b3b6",
   "metadata": {},
   "source": [
    "### Compare both runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f616504",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _moving_average(arr, window):\n",
    "    \"\"\"Simple moving average that keeps the original length.\"\"\"\n",
    "    if window <= 1:\n",
    "        return np.asarray(arr)\n",
    "    cumsum = np.cumsum(np.insert(arr, 0, 0))\n",
    "    smoothed = (cumsum[window:] - cumsum[:-window]) / float(window)\n",
    "    # pad the left side so lengths match\n",
    "    left_pad = np.full(window - 1, smoothed[0])\n",
    "    return np.concatenate([left_pad, smoothed])\n",
    "\n",
    "def compare_error_histories(\n",
    "    runs,\n",
    "    labels=None,\n",
    "    backend=\"plotly\",\n",
    "    fig_size=(900, 500),\n",
    "    smooth=True,\n",
    "    smooth_window=50,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare relative and max error histories from multiple runs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    runs : list of dict – each dict must contain keys\n",
    "           'l2_rel_err', 'max_err'\n",
    "    labels : list of str – legend labels, default \"Run 1\", \"Run 2\", ...\n",
    "    \"\"\"\n",
    "\n",
    "    n_runs = len(runs)\n",
    "    assert n_runs > 0, \"runs list cannot be empty\"\n",
    "    if labels is None:\n",
    "        labels = [f\"Run {i+1}\" for i in range(n_runs)]\n",
    "    assert len(labels) == n_runs, \"`labels` length must match `runs` length\"\n",
    "\n",
    "    colors = {\"rel_err\": \"#d62728\", \"max_err\": \"#2ca02c\"}\n",
    "    dashes = [\"solid\", \"dash\", \"dot\", \"dashdot\", \"longdash\", \"longdashdot\"]\n",
    "\n",
    "    def prep(d):\n",
    "        rel = np.asarray(d[\"l2_rel_err\"])\n",
    "        mx = np.asarray(d[\"max_err\"])\n",
    "        if smooth:\n",
    "            rel, mx = (_moving_average(x, smooth_window) for x in (rel, mx))\n",
    "        return rel, mx\n",
    "\n",
    "    processed = [prep(r) for r in runs]\n",
    "    x = np.arange(len(processed[0][0]))  # assume equal length\n",
    "\n",
    "    # --- Plotly backend ---\n",
    "    if backend.lower() == \"plotly\":\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.07,\n",
    "            subplot_titles=(\"L2 Relative Error\", \"Maximum Error\")\n",
    "        )\n",
    "\n",
    "        def add(row, y_values, color, dash, label, showlegend):\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=x, y=y_values, mode=\"lines\",\n",
    "                    name=label,\n",
    "                    line=dict(color=color, dash=dash)\n",
    "                ),\n",
    "                row=row, col=1,\n",
    "            )\n",
    "\n",
    "        for run_idx, (rel, mx) in enumerate(processed):\n",
    "            dash = dashes[run_idx % len(dashes)]\n",
    "            label = labels[run_idx]\n",
    "            show = (run_idx == 0)\n",
    "            add(1, rel, colors[\"rel_err\"], dash, label, show)\n",
    "            add(2, mx, colors[\"max_err\"], dash, label, False)\n",
    "\n",
    "        fig.update_yaxes(type=\"log\")\n",
    "        fig.update_layout(\n",
    "            height=fig_size[1], width=fig_size[0],\n",
    "            title_text=\"Error Comparison\",\n",
    "            legend_title=\"Run\"\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "    # --- Matplotlib backend ---\n",
    "    else:\n",
    "        _, axes = plt.subplots(2, 1,\n",
    "                               figsize=(fig_size[0] / 100, fig_size[1] / 100),\n",
    "                               sharex=True)\n",
    "\n",
    "        titles = [\"L2 Relative Error\", \"Maximum Error\"]\n",
    "\n",
    "        for run_idx, (rel, mx) in enumerate(processed):\n",
    "            dash = dashes[run_idx % len(dashes)]\n",
    "            label = labels[run_idx]\n",
    "            axes[0].plot(x, rel, label=label,\n",
    "                         color=colors[\"rel_err\"], linestyle=dash)\n",
    "            axes[1].plot(x, mx, label=label,\n",
    "                         color=colors[\"max_err\"], linestyle=dash)\n",
    "\n",
    "        for ax, title in zip(axes, titles):\n",
    "            ax.set_yscale(\"log\")\n",
    "            ax.set_title(title, fontsize=10)\n",
    "\n",
    "        axes[-1].set_xlabel(\"Epoch\")\n",
    "        axes[0].set_ylabel(\"Relative Error (log)\")\n",
    "        axes[1].set_ylabel(\"Max Error (log)\")\n",
    "        axes[0].legend(loc=\"upper right\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66a6e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_error_histories(\n",
    "    [with_vanilla, with_spinn, with_anzats],\n",
    "    labels=[\"Vanilla\", \"SPINN\", \"NN+Anzatz\"],\n",
    "    smooth=True,\n",
    "    smooth_window=10,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
